{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with RNN\n",
    "\n",
    "We apply RNN to the language translation task, which is a typical seq to seq learning tastk. We will use a Encoder-Decoder architecture, where we use RNN models for both Encoder and Decoder parts. \n",
    "\n",
    "We will use the data from [the Tatoeba Project](http://www.manythings.org/anki/) and specifically, we will start with the fra-eng (French-English) translation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from random import shuffle, sample\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw texts and perform train-val-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197463\n"
     ]
    }
   ],
   "source": [
    "## Read all lines and shuffle\n",
    "file = open(\"datasets/fra-eng/fra.txt\", \"r\")\n",
    "all_lines = file.readlines()\n",
    "shuffle(all_lines)\n",
    "#all_lines = sample(all_lines)\n",
    "print(len(all_lines))\n",
    "## Train-Val-Test split\n",
    "train_ratio, val_ratio = 0.8, 0.1\n",
    "train_end = int(len(all_lines)*train_ratio)\n",
    "val_end = int(len(all_lines)*(train_ratio+val_ratio))\n",
    "train_raw = all_lines[:train_end]\n",
    "val_raw = all_lines[train_end:val_end]\n",
    "test_raw = all_lines[val_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab, Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(lines, source_tokenizer, target_tokenizer):\n",
    "    source_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    for line in lines:\n",
    "        parts = line.split(\"\\t\") #source, target and note are separated by tab\n",
    "        source_counter.update(source_tokenizer(parts[0].lower()))\n",
    "        target_counter.update(target_tokenizer(parts[1].lower()))\n",
    "    source_vocab = vocab(source_counter, specials=[\"<unk>\",\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    target_vocab = vocab(target_counter, specials=[\"<unk>\",\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    source_vocab.set_default_index(source_vocab['<unk>'])\n",
    "    target_vocab.set_default_index(target_vocab['<unk>'])    \n",
    "    return source_vocab, target_vocab\n",
    "\n",
    "def process_data(lines, source_tokenizer, target_tokenizer, source_vocab, target_vocab):\n",
    "    data=[]\n",
    "    for line in lines:\n",
    "        source, target, _ = line.split(\"\\t\")\n",
    "        source_tensor = torch.tensor(source_vocab(source_tokenizer(source.lower())))\n",
    "        target_tensor = torch.tensor(target_vocab(target_tokenizer(target.lower())))\n",
    "        data.append((source_tensor, target_tensor))\n",
    "    return data\n",
    "\n",
    "def generate_batch(data_batch, source_vocab, target_vocab):\n",
    "    source_batch = []\n",
    "    target_batch = []\n",
    "    for source, target in data_batch:\n",
    "        source_batch.append(torch.cat([torch.tensor([source_vocab[\"<bos>\"]]), source, torch.tensor([source_vocab[\"<eos>\"]])], dim=0))\n",
    "        target_batch.append(torch.cat([torch.tensor([target_vocab[\"<bos>\"]]), target, torch.tensor([target_vocab[\"<eos>\"]])], dim=0))\n",
    "    source_batch = pad_sequence(source_batch, padding_value=source_vocab[\"<pad>\"])\n",
    "    target_batch = pad_sequence(target_batch, padding_value=target_vocab[\"<pad>\"])\n",
    "    return source_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use tokenizers from spacy\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", language=\"fr_core_news_sm\")\n",
    "## Get source target vocabs\n",
    "en_vocab, fr_vocab = get_vocabs(train_raw, en_tokenizer, fr_tokenizer)\n",
    "## Get source tartget token ids\n",
    "train_data = process_data(train_raw, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "val_data = process_data(val_raw, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "test_data = process_data(test_raw, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "## Get train, val, test dataloader\n",
    "collator = lambda x: generate_batch(x, en_vocab, fr_vocab)\n",
    "bsz = 16\n",
    "train_iter = DataLoader(train_data, batch_size=bsz, shuffle=True, collate_fn=collator)\n",
    "val_iter = DataLoader(val_data, batch_size=bsz, shuffle=False, collate_fn=collator)\n",
    "test_iter = DataLoader(test_data, batch_size=bsz, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Seq2Seq Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_dim, self.emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(2*enc_hid_dim, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embeded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embeded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat([hidden[-2,:,:],hidden[-1,:,:]],dim=-1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hid_dim, dec_hid_dim, attn_dim):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim*2) + dec_hid_dim \n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "\n",
    "        energy = torch.tanh(\n",
    "            self.attn(torch.cat([repeated_decoder_hidden, encoder_outputs], dim=-1))\n",
    "        )\n",
    "        attention = torch.sum(energy, dim=-1)\n",
    "        return F.softmax(attention, dim=-1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hid_dim*2+emb_dim, dec_hid_dim)\n",
    "        self.out = nn.Linear(self.attention.attn_in+emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _weighted_encoder_rep(self, decoder_hidden, encoder_outputs):\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1,0,2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "    \n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embeded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embeded, weighted_encoder_rep),dim=2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embeded = embeded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output, weighted_encoder_rep, embeded),dim=1))\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1] # tensor.max() returns tuple of maxiums and idmax\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "emb_dim = 32\n",
    "enc_hid_dim = 64\n",
    "dec_hid_dim = 64\n",
    "dropout = 0.8\n",
    "encoder = Encoder(input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout)\n",
    "attn_dim = 8\n",
    "attention = Attention(enc_hid_dim, dec_hid_dim, attn_dim)\n",
    "output_dim = len(fr_vocab)\n",
    "decoder = Decoder(output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention)\n",
    "\n",
    "##device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "device = torch.device(\"cuda\")## Only works on NVIDIA devices\n",
    "model = Seq2Seq(encoder, decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(14535, 32)\n",
       "    (rnn): GRU(32, 64, bidirectional=True)\n",
       "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(24845, 32)\n",
       "    (rnn): GRU(160, 64)\n",
       "    (out): Linear(in_features=224, out_features=24845, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Evaluation, Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:,].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def evaluate_step(model, iterator, criterion):\n",
    "    model.eval\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:,].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "\n",
    "def train(model, train_iter, val_iter, n_epochs, ignore_index, clip=1):\n",
    "    optimizer  = optim.Adam(model.parameters())\n",
    "    loss = nn.CrossEntropyLoss(ignore_index = ignore_index)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_step(model, train_iter, optimizer, loss, clip)\n",
    "        val_loss = evaluate_step(model, val_iter, loss)\n",
    "        print(f\"Epoch: {epoch+1}-----------------------------------------------------\")\n",
    "        print(f\"\\tTrain Loss: {train_loss} | Train PPL:{math.exp(train_loss)}\")\n",
    "        print(f\"\\tVal.  Loss: {val_loss} | Val. PPL:{math.exp(val_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, train_iter,val_iter, \u001b[39m2\u001b[39;49m, fr_vocab[\u001b[39m\"\u001b[39;49m\u001b[39m<pad>\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn [48], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, val_iter, n_epochs, ignore_index, clip)\u001b[0m\n\u001b[0;32m     43\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index \u001b[39m=\u001b[39m ignore_index)\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m---> 45\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model, train_iter, optimizer, loss, clip)\n\u001b[0;32m     46\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate_step(model, val_iter, loss)\n\u001b[0;32m     47\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-----------------------------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [48], line 7\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i, (src, trg) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(iterator):\n\u001b[0;32m      6\u001b[0m     src, trg \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m     output \u001b[39m=\u001b[39m model(src, trg)\n\u001b[0;32m      9\u001b[0m     output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m:,]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     10\u001b[0m     trg \u001b[39m=\u001b[39m trg[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\guangweiweng\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [29], line 103\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m    100\u001b[0m output \u001b[39m=\u001b[39m trg[\u001b[39m0\u001b[39m,:]\n\u001b[0;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_len):\n\u001b[1;32m--> 103\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(output, hidden, encoder_outputs)\n\u001b[0;32m    104\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n\u001b[0;32m    105\u001b[0m     teacher_force \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m teacher_forcing_ratio\n",
      "File \u001b[1;32mc:\\Users\\guangweiweng\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [29], line 77\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, input, decoder_hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     73\u001b[0m rnn_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((embeded, weighted_encoder_rep),dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     75\u001b[0m output, decoder_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(rnn_input, decoder_hidden\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m---> 77\u001b[0m embeded \u001b[39m=\u001b[39m embeded\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     78\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     79\u001b[0m weighted_encoder_rep \u001b[39m=\u001b[39m weighted_encoder_rep\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_iter,val_iter, 2, fr_vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28cdd387f5a19c1a019582c35e0554b941f30c5a12df8630a85901c6e2e83244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
