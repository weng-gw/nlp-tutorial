{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with RNN\n",
    "\n",
    "We apply RNN to the language translation task, which is a typical seq to seq learning tastk. We will use a Encoder-Decoder architecture, where we use RNN models for both Encoder and Decoder parts. \n",
    "\n",
    "We will use the data from [the Tatoeba Project](http://www.manythings.org/anki/) and specifically, we will start with the fra-eng (French-English) translation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from random import shuffle, sample\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw texts and perform train-val-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read all lines and shuffle\n",
    "file = open(\"datasets/fra-eng/fra.txt\", \"r\")\n",
    "all_lines = file.readlines()\n",
    "shuffle(all_lines)\n",
    "all_lines = sample(all_lines, 1000)\n",
    "## Train-Val-Test split\n",
    "train_ratio, val_ratio = 0.8, 0.1\n",
    "train_end = int(len(all_lines)*train_ratio)\n",
    "val_end = int(len(all_lines)*(train_ratio+val_ratio))\n",
    "train_raw = all_lines[:train_end]\n",
    "val_raw = all_lines[train_end:val_end]\n",
    "test_raw = all_lines[val_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab, Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(lines, source_tokenizer, target_tokenizer):\n",
    "    source_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    for line in lines:\n",
    "        parts = line.split(\"\\t\") #source, target and note are separated by tab\n",
    "        source_counter.update(source_tokenizer(parts[0].lower()))\n",
    "        target_counter.update(target_tokenizer(parts[1].lower()))\n",
    "    source_vocab = vocab(source_counter, specials=[\"<unk>\",\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    target_vocab = vocab(target_counter, specials=[\"<unk>\",\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    source_vocab.set_default_index(source_vocab['<unk>'])\n",
    "    target_vocab.set_default_index(target_vocab['<unk>'])    \n",
    "    return source_vocab, target_vocab\n",
    "\n",
    "def process_data(lines, source_tokenizer, target_tokenizer, source_vocab, target_vocab):\n",
    "    data=[]\n",
    "    for line in lines:\n",
    "        source, target, _ = line.split(\"\\t\")\n",
    "        source_tensor = torch.tensor(source_vocab(source_tokenizer(source.lower())))\n",
    "        target_tensor = torch.tensor(target_vocab(target_tokenizer(target.lower())))\n",
    "        data.append((source_tensor, target_tensor))\n",
    "    return data\n",
    "\n",
    "def generate_batch(data_batch, source_vocab, target_vocab):\n",
    "    source_batch = []\n",
    "    target_batch = []\n",
    "    for source, target in data_batch:\n",
    "        source_batch.append(torch.cat([torch.tensor([source_vocab[\"<bos>\"]]), source, torch.tensor([source_vocab[\"<eos>\"]])], dim=0))\n",
    "        target_batch.append(torch.cat([torch.tensor([target_vocab[\"<bos>\"]]), target, torch.tensor([target_vocab[\"<eos>\"]])], dim=0))\n",
    "    source_batch = pad_sequence(source_batch, padding_value=source_vocab[\"<pad>\"])\n",
    "    target_batch = pad_sequence(target_batch, padding_value=target_vocab[\"<pad>\"])\n",
    "    return source_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use tokenizers from spacy\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", language=\"fr_core_news_sm\")\n",
    "## Get source target vocabs\n",
    "en_vocab, fr_vocab = get_vocabs(train_raw, en_tokenizer, fr_tokenizer)\n",
    "## Get source tartget token ids\n",
    "train_data = process_data(train_raw, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "val_data = process_data(val_raw, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "test_data = process_data(test_raw, en_tokenizer, fr_tokenizer, en_vocab, fr_vocab)\n",
    "## Get train, val, test dataloader\n",
    "collator = lambda x: generate_batch(x, en_vocab, fr_vocab)\n",
    "bsz = 16\n",
    "train_iter = DataLoader(train_data, batch_size=bsz, shuffle=True, collate_fn=collator)\n",
    "val_iter = DataLoader(val_data, batch_size=bsz, shuffle=False, collate_fn=collator)\n",
    "test_iter = DataLoader(test_data, batch_size=bsz, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Seq2Seq Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_dim, self.emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(2*enc_hid_dim, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embeded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embeded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat([hidden[-2,:,:],hidden[-1,:,:]],dim=-1)))\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hid_dim, dec_hid_dim, attn_dim):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim*2) + dec_hid_dim \n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "\n",
    "        energy = torch.tanh(\n",
    "            self.attn(torch.cat([repeated_decoder_hidden, encoder_outputs], dim=-1))\n",
    "        )\n",
    "        attention = torch.sum(energy, dim=-1)\n",
    "        return F.softmax(attention, dim=-1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hid_dim*2+emb_dim, dec_hid_dim)\n",
    "        self.out = nn.Linear(self.attention.attn_in+emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _weighted_encoder_rep(self, decoder_hidden, encoder_outputs):\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1,0,2)\n",
    "\n",
    "        return weighted_encoder_rep\n",
    "    \n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embeded = self.dropout(self.embedding(input))\n",
    "\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embeded, weighted_encoder_rep),dim=2)\n",
    "\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embeded = embeded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output, weighted_encoder_rep, embeded),dim=1))\n",
    "        return output, decoder_hidden.squeeze(0)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1] # tensor.max() returns tuple of maxiums and idmax\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "emb_dim = 32\n",
    "enc_hid_dim = 64\n",
    "dec_hid_dim = 64\n",
    "dropout = 0.8\n",
    "encoder = Encoder(input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout)\n",
    "attn_dim = 8\n",
    "attention = Attention(enc_hid_dim, dec_hid_dim, attn_dim)\n",
    "output_dim = len(fr_vocab)\n",
    "decoder = Decoder(output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "##device = torch.device(\"cpu\") \n",
    "model = Seq2Seq(encoder, decoder,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(1186, 32)\n",
       "    (rnn): GRU(32, 64, bidirectional=True)\n",
       "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(1483, 32)\n",
       "    (rnn): GRU(160, 64)\n",
       "    (out): Linear(in_features=224, out_features=1483, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Evaluation, Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:,].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "def evaluate_step(model, iterator, criterion):\n",
    "    model.eval\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:,].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "\n",
    "def train(model, train_iter, val_iter, n_epochs, ignore_index, clip=1):\n",
    "    optimizer  = optim.Adam(model.parameters())\n",
    "    loss = nn.CrossEntropyLoss(ignore_index = ignore_index)\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = train_step(model, train_iter, optimizer, loss, clip)\n",
    "        val_loss = evaluate_step(model, val_iter, loss)\n",
    "        print(f\"Epoch: {epoch+1}-----------------------------------------------------\")\n",
    "        print(f\"\\tTrain Loss: {train_loss} | Train PPL:{math.exp(train_loss)}\")\n",
    "        print(f\"\\tVal.  Loss: {val_loss} | Val. PPL:{math.exp(val_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1-----------------------------------------------------\n",
      "\tTrain Loss: 6.752014045715332 | Train PPL:855.7806092744532\n",
      "\tVal.  Loss: 6.067500182560512 | Train PPL:431.60040966478596\n",
      "Epoch: 2-----------------------------------------------------\n",
      "\tTrain Loss: 5.8699533653259275 | Train PPL:354.23246036723316\n",
      "\tVal.  Loss: 6.121813501630511 | Train PPL:455.69034079383084\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter,val_iter, 2, fr_vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
