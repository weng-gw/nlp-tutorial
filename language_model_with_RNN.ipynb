{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model with RNN\n",
    "\n",
    "In this notebook, we will build language model (sequence model) with RNN. We can think of it as a \"deep\" version of the classical N-gram language model. The trained model will allow us to generate sentences given the words (prefixes) we provided.\n",
    "\n",
    "We will use [The Time Machine](https://www.gutenberg.org/files/35/35-0.txt) as used in D2L to train the model. This si a faily small corpus of just over 30000 words.\n",
    "\n",
    "The entire process will follow the below steps:\n",
    "1. Read the dataset, preprocess to remove spicial characters, tokenize the text, build vocabulary and transform tokens into ids.\n",
    "2. Build data iterator for modeling training.\n",
    "3. Create RNN modules\n",
    "4. Building training and prediction steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, tokenization, vocabulary building and token transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.transforms import VocabTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text \n",
    "def read_time_machine():\n",
    "    with open(\"datasets/the_time_machine.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+',' ', line).strip().lower()  for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token list iterator for generating vocab\n",
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(min_freq = 1, max_tokens = None):\n",
    "    lines = read_time_machine()\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines,tokenizer), specials=[\"<unk>\"], min_freq = min_freq, max_tokens=max_tokens)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    vocab_transform = VocabTransform(vocab)\n",
    "    corpus = vocab_transform(tokenizer(\" \".join(lines)))\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus_time_machine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randon sampling: In random sampling, each example is a subsequence arbitrarily captured on the original long sequence.\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    corpus = corpus[random.randint(0,num_steps-1):] #include random offset at the beginning\n",
    "    num_subseqs = (len(corpus)-1)//num_steps  #Subtract 1 since we need to account for labels\n",
    "    initial_indices = list(range(0, num_subseqs*num_steps, num_steps))\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        return corpus[pos:pos+num_steps]\n",
    "    num_batches = num_subseqs//batch_size\n",
    "    for i in range(0, batch_size*num_batches, num_batches):\n",
    "        initial_indices_per_batch = initial_indices[i:i+batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j+1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[11, 12, 13, 14, 15],\n",
      "        [21, 22, 23, 24, 25]]) \n",
      "Y: tensor([[12, 13, 14, 15, 16],\n",
      "        [22, 23, 24, 25, 26]])\n",
      "X: tensor([[16, 17, 18, 19, 20],\n",
      "        [26, 27, 28, 29, 30]]) \n",
      "Y: tensor([[17, 18, 19, 20, 21],\n",
      "        [27, 28, 29, 30, 31]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq,batch_size=2, num_steps=5):\n",
    "    print(\"X:\",X,\"\\nY:\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential Partitioning: In this partitioning, we ensure that the subsequences from two adjacent minibatches during iteration \n",
    "## are adjacent on the original sequence.\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus)-offset-1)//batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset+num_tokens]).reshape(batch_size,-1)\n",
    "    Ys = torch.tensor(corpus[offset+1: offset+1+num_tokens]).reshape(batch_size,-1)\n",
    "    num_batches = Xs.shape[1]// num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i:i+num_steps]\n",
    "        Y = Ys[:, i:i+num_steps]\n",
    "        yield X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [17, 18, 19, 20, 21]]) \n",
      "Y: tensor([[ 2,  3,  4,  5,  6],\n",
      "        [18, 19, 20, 21, 22]])\n",
      "X: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [22, 23, 24, 25, 26]]) \n",
      "Y: tensor([[ 7,  8,  9, 10, 11],\n",
      "        [23, 24, 25, 26, 27]])\n",
      "X: tensor([[11, 12, 13, 14, 15],\n",
      "        [27, 28, 29, 30, 31]]) \n",
      "Y: tensor([[12, 13, 14, 15, 16],\n",
      "        [28, 29, 30, 31, 32]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_sequential(my_seq,batch_size=2, num_steps=5):\n",
    "    print(\"X:\",X,\"\\nY:\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    def __init__(self, corpus, batch_size, num_steps, use_random_iter):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus = corpus\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps, use_random_iter = False, max_tokens = None, min_freq=1):\n",
    "    corpus, vocab = load_corpus_time_machine(min_freq = min_freq, max_tokens = max_tokens)\n",
    "    data_iter = SeqDataLoader(corpus, batch_size, num_steps, use_random_iter)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = load_data_time_machine(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size, embed_size=-1):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        if embed_size >=0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
    "        else:\n",
    "            self.embedding_layer = lambda X: F.one_hot(X.T.long(), vocab_size)\n",
    "        \n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens*2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = self.embedding_layer(inputs)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, device, batch_size =1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            return torch.zeros((self.num_directions*self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens), device=device)\n",
    "        else:\n",
    "            return (torch.zeros((self.num_directions*self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions*self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_pred(prefix, num_preds, net, vocab, device):\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_inputs = lambda: torch.tensor([outputs[-1]],device=device).reshape((1,1))\n",
    "    for y in prefix[1:]:\n",
    "        _, state = net(get_inputs(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_inputs(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ' '.join([vocab.get_itos()[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hidden_size = 256\n",
    "rnn_layer = nn.RNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "net = RNNModel(rnn_layer, vocab_size=vocab_size)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my <unk> has dimly swamp returned stuffy taken pathway expect decorations casting expect'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pred([\"my\", \"wife\", \"has\"], 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, optimizer, device, use_random_iter=False):\n",
    "    state = None\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
