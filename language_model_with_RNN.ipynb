{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model with RNN\n",
    "\n",
    "In this notebook, we will build language model (sequence model) with RNN. We can think of it as a \"deep\" version of the classical N-gram language model. The trained model will allow us to generate sentences given the words (prefixes) we provided.\n",
    "\n",
    "We will use [The Time Machine](https://www.gutenberg.org/files/35/35-0.txt) as used in D2L to train the model. This si a faily small corpus of just over 30000 words.\n",
    "\n",
    "The entire process will follow the below steps:\n",
    "1. Read the dataset, preprocess to remove spicial characters, tokenize the text, build vocabulary and transform tokens into ids.\n",
    "2. Build data iterator for modeling training.\n",
    "3. Create RNN modules\n",
    "4. Building training and prediction steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, tokenization, vocabulary building and token transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.transforms import VocabTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text \n",
    "def read_time_machine():\n",
    "    with open(\"datasets/the_time_machine.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+',' ', line).strip().lower()  for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token list iterator for generating vocab\n",
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(min_freq = 1, max_tokens = None):\n",
    "    lines = read_time_machine()\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines,tokenizer), specials=[\"<unk>\"], min_freq = min_freq, max_tokens=max_tokens)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    vocab_transform = VocabTransform(vocab)\n",
    "    corpus = vocab_transform(tokenizer(\" \".join(lines)))\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus_time_machine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randon sampling: In random sampling, each example is a subsequence arbitrarily captured on the original long sequence.\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    corpus = corpus[random.randint(0,num_steps-1):] #include random offset at the beginning\n",
    "    num_subseqs = (len(corpus)-1)//num_steps  #Subtract 1 since we need to account for labels\n",
    "    initial_indices = list(range(0, num_subseqs*num_steps, num_steps))\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        return corpus[pos:pos+num_steps]\n",
    "    num_batches = num_subseqs//batch_size\n",
    "    for i in range(0, batch_size*num_batches, num_batches):\n",
    "        initial_indices_per_batch = initial_indices[i:i+batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j+1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4]]) \n",
      "Y: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [ 1,  2,  3,  4,  5]])\n",
      "X: tensor([[25, 26, 27, 28, 29],\n",
      "        [10, 11, 12, 13, 14]]) \n",
      "Y: tensor([[26, 27, 28, 29, 30],\n",
      "        [11, 12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq,batch_size=2, num_steps=5):\n",
    "    print(\"X:\",X,\"\\nY:\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sequential Partitioning: In this partitioning, we ensure that the subsequences from two adjacent minibatches during iteration \n",
    "## are adjacent on the original sequence.\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus)-offset-1)//batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset+num_tokens]).reshape(batch_size,-1)\n",
    "    Ys = torch.tensor(corpus[offset+1: offset+1+num_tokens]).reshape(batch_size,-1)\n",
    "    num_batches = Xs.shape[1]// num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i:i+num_steps]\n",
    "        Y = Ys[:, i:i+num_steps]\n",
    "        yield X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 5,  6,  7,  8,  9],\n",
      "        [19, 20, 21, 22, 23]]) \n",
      "Y: tensor([[ 6,  7,  8,  9, 10],\n",
      "        [20, 21, 22, 23, 24]])\n",
      "X: tensor([[10, 11, 12, 13, 14],\n",
      "        [24, 25, 26, 27, 28]]) \n",
      "Y: tensor([[11, 12, 13, 14, 15],\n",
      "        [25, 26, 27, 28, 29]])\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_sequential(my_seq,batch_size=2, num_steps=5):\n",
    "    print(\"X:\",X,\"\\nY:\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader:\n",
    "    def __init__(self, corpus, batch_size, num_steps, use_random_iter):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus = corpus\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps, use_random_iter = False, max_tokens = None, min_freq=1):\n",
    "    corpus, vocab = load_corpus_time_machine(min_freq = min_freq, max_tokens = max_tokens)\n",
    "    data_iter = SeqDataLoader(corpus, batch_size, num_steps, use_random_iter)\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocab = load_data_time_machine(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size, embed_size=-1):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        if embed_size >=0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
    "        else:\n",
    "            self.embedding_layer = lambda X: F.one_hot(X.T.long(), vocab_size)\n",
    "        \n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens*2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = self.embedding_layer(inputs)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, device, batch_size =1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            return torch.zeros((self.num_directions*self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens), device=device)\n",
    "        else:\n",
    "            return (torch.zeros((self.num_directions*self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions*self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_pred(prefix, num_preds, net, vocab, device):\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_inputs = lambda: torch.tensor([outputs[-1]],device=device).reshape((1,1))\n",
    "    for y in prefix[1:]:\n",
    "        _, state = net(get_inputs(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_inputs(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ' '.join([vocab.get_itos()[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hidden_size = 256\n",
    "rnn_layer = nn.RNN(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "net = RNNModel(rnn_layer, vocab_size=vocab_size)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my <unk> has the the the the the the the the the the'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pred([\"my\", \"wife\", \"has\"], 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_epoch(net, train_iter, loss, optimizer, device, use_random_iter=False):\n",
    "    state = None\n",
    "    metric = 0\n",
    "    N =0\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size = X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat,y.long()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        metric += float(l*y.numel())\n",
    "        N += int(y.numel())\n",
    "        ##print(f\"Loss: {float(l)}\")\n",
    "    return metric/N\n",
    "\n",
    "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    predict = lambda prefix: rnn_pred(prefix.split(), 20, net, vocab, device)\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch(net, data_iter, loss, optimizer, device, use_random_iter)\n",
    "        if (epoch+1)%50==0:\n",
    "            print(f\"epoch {epoch}: perplexity {ppl}\")\n",
    "            print(predict(\"time traveller\"))\n",
    "    print(predict(\"time traveller\"))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49: perplexity 6.293149941184304\n",
      "time traveller the the the the the the the the the the the the the the the the the the the the\n",
      "epoch 99: perplexity 5.937831587357954\n",
      "time traveller the the of the the of the the of the the of the the of the the of the the\n",
      "epoch 149: perplexity 5.583666354092685\n",
      "time traveller the the the the the the the the the the the the the the the the the the the the\n",
      "epoch 199: perplexity 4.776057184392756\n",
      "time traveller the time traveller the time traveller the time traveller the time traveller the time traveller the time traveller the time\n",
      "epoch 249: perplexity 5.167464405406605\n",
      "time traveller the time traveller the time traveller the time traveller the time traveller the time traveller the time traveller the time\n",
      "epoch 299: perplexity 4.709211009632457\n",
      "time traveller the and the the the the the the the the the the the the the the the the the the\n",
      "epoch 349: perplexity 4.388375632546165\n",
      "time traveller the time traveller the time traveller the time traveller the time traveller the time traveller the time traveller the time\n",
      "epoch 399: perplexity 3.551113517067649\n",
      "time traveller then when i had seen it was heavens i was the smile i done to fill to come from the\n",
      "epoch 449: perplexity 4.156028629649769\n",
      "time traveller then when i had seen the s the time traveller the time traveller smiled are you may copy the time\n",
      "epoch 499: perplexity 3.2032388617775656\n",
      "time traveller then when i could not and young man a fourth and a minute or so and i of the the\n",
      "epoch 549: perplexity 2.9284494954889473\n",
      "time traveller then when i had the the simple explanation i had mastered the problem of the world mastered the whole secret\n",
      "epoch 599: perplexity 3.4579169672185723\n",
      "time traveller then when i had not it man in the or said it should think and minute i a minute perhaps\n",
      "epoch 649: perplexity 3.3570974211259323\n",
      "time traveller then when i had the the s looking contact held i had said medical man he said now i very\n",
      "epoch 699: perplexity 3.099168820814653\n",
      "time traveller then when we had seen the editor and time him he said and the as of when i i not\n",
      "epoch 749: perplexity 3.058321951085871\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n",
      "epoch 799: perplexity 2.331795196533203\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n",
      "epoch 849: perplexity 2.307781207344749\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n",
      "epoch 899: perplexity 3.1625532887198706\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n",
      "epoch 949: perplexity 2.875433961694891\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n",
      "epoch 999: perplexity 2.457117420543324\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n",
      "time traveller then when we had all imitated the action of the medical man he said now i want you clearly to\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "vocab_size = len(vocab)\n",
    "hidden_size = 256\n",
    "rnn_layer = nn.LSTM(vocab_size, hidden_size)\n",
    "net = RNNModel(rnn_layer, vocab_size=vocab_size)\n",
    "net = net.to(device)\n",
    "\n",
    "data_iter, vocab = load_data_time_machine(32, 50)\n",
    "train(net, data_iter, vocab, 0.001, 1000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i would like to the this but i saw the i was thinking of the earth here and there came the sharp vertical line of some cupola or obelisk there were no hedges no'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pred(\"i would like to\".split(), 30, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
