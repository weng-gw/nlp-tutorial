{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with TextCNN\n",
    "We will use CNN over text embeddings to performance text classification. We can think of textCNN as a deep version of n-gram.\n",
    "See [Section 15.3 of D2L](https://classic.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.transforms import VocabTransform, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = \"datasets/twitter_sentiment_analysis/twitter_training.csv\"\n",
    "train_data = pd.read_csv(train_data_path,header=None)\n",
    "train_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]\n",
    "\n",
    "test_data_path = \"datasets/twitter_sentiment_analysis/twitter_validation.csv\"\n",
    "test_data = pd.read_csv(test_data_path,header=None)\n",
    "test_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]\n",
    "\n",
    "train_data.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_to_label = {\"Negative\":0, \"Irrelevant\":1, \"Neutral\":2, \"Positive\": 3}\n",
    "train_data[\"label\"] = train_data.sentiment.map(sentiment_to_label)\n",
    "test_data[\"label\"] = test_data.sentiment.map(sentiment_to_label)\n",
    "\n",
    "train_data.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset:\n",
    "    def __init__(self, texts, label):\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.label[idx]\n",
    "\n",
    "train_dataset = TwitterDataset(train_data['Tweet_content'].map(str).values,train_data[\"label\"].values)\n",
    "test_dataset = TwitterDataset(test_data['Tweet_content'].map(str).values,test_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data[\"Tweet_content\"].map(str)), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "totensor = ToTensor(padding_value=vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We redefine the collate function so that it will do dynamic padding\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, text_lens = [], [], []\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = vocab(tokenizer(_text))\n",
    "        text_list.append(processed_text)\n",
    "        text_lens.append(len(processed_text))\n",
    "    label_list = torch.tensor(label_list,dtype=torch.int64)\n",
    "    text_list=  totensor(text_list)\n",
    "    text_lens = torch.tensor(text_lens,dtype = torch.int64)\n",
    "    return  text_list, label_list, text_lens\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 55])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "X, y, l = next(iter(train_dataloader))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building textCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels, n_class=4):\n",
    "        super(TextCNN,self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels),n_class)\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(embed_size,c,k))\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0,2,1)\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1\n",
    "        )\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(len(vocab),64,[3,4,5],[20,20,20]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, print_per_batches=200):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y, l) in enumerate(dataloader):\n",
    "        X, y, l  = X.to(device), y.to(device), l.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_per_batches ==0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y, l) in enumerate(dataloader):\n",
    "            X, y, l = X.to(device), y.to(device), l.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred,y)\n",
    "            test_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.607651  [    0/74682]\n",
      "loss: 1.340992  [ 6400/74682]\n",
      "loss: 1.331274  [12800/74682]\n",
      "loss: 1.212045  [19200/74682]\n",
      "loss: 1.295337  [25600/74682]\n",
      "loss: 1.177739  [32000/74682]\n",
      "loss: 1.240867  [38400/74682]\n",
      "loss: 1.367395  [44800/74682]\n",
      "loss: 1.226619  [51200/74682]\n",
      "loss: 1.096165  [57600/74682]\n",
      "loss: 1.104173  [64000/74682]\n",
      "loss: 1.133745  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 0.969664 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.166240  [    0/74682]\n",
      "loss: 1.059350  [ 6400/74682]\n",
      "loss: 1.082567  [12800/74682]\n",
      "loss: 0.938266  [19200/74682]\n",
      "loss: 1.120689  [25600/74682]\n",
      "loss: 1.082127  [32000/74682]\n",
      "loss: 1.001250  [38400/74682]\n",
      "loss: 1.128776  [44800/74682]\n",
      "loss: 0.744910  [51200/74682]\n",
      "loss: 0.789417  [57600/74682]\n",
      "loss: 0.872674  [64000/74682]\n",
      "loss: 0.941402  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.756742 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.876672  [    0/74682]\n",
      "loss: 0.915215  [ 6400/74682]\n",
      "loss: 1.091853  [12800/74682]\n",
      "loss: 0.980577  [19200/74682]\n",
      "loss: 0.886278  [25600/74682]\n",
      "loss: 0.819305  [32000/74682]\n",
      "loss: 0.837727  [38400/74682]\n",
      "loss: 0.868656  [44800/74682]\n",
      "loss: 0.848389  [51200/74682]\n",
      "loss: 0.814311  [57600/74682]\n",
      "loss: 0.769435  [64000/74682]\n",
      "loss: 0.672400  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.546231 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.490990  [    0/74682]\n",
      "loss: 0.556409  [ 6400/74682]\n",
      "loss: 0.626319  [12800/74682]\n",
      "loss: 0.510926  [19200/74682]\n",
      "loss: 0.661568  [25600/74682]\n",
      "loss: 0.610250  [32000/74682]\n",
      "loss: 1.008806  [38400/74682]\n",
      "loss: 0.658011  [44800/74682]\n",
      "loss: 0.485460  [51200/74682]\n",
      "loss: 0.838940  [57600/74682]\n",
      "loss: 0.700876  [64000/74682]\n",
      "loss: 0.920212  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.400995 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.725089  [    0/74682]\n",
      "loss: 0.652265  [ 6400/74682]\n",
      "loss: 0.512566  [12800/74682]\n",
      "loss: 0.583158  [19200/74682]\n",
      "loss: 0.573171  [25600/74682]\n",
      "loss: 0.684139  [32000/74682]\n",
      "loss: 0.592799  [38400/74682]\n",
      "loss: 0.762681  [44800/74682]\n",
      "loss: 0.481797  [51200/74682]\n",
      "loss: 0.654044  [57600/74682]\n",
      "loss: 0.924926  [64000/74682]\n",
      "loss: 0.521985  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 90.7%, Avg loss: 0.296550 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.718239  [    0/74682]\n",
      "loss: 0.578771  [ 6400/74682]\n",
      "loss: 0.372939  [12800/74682]\n",
      "loss: 0.328271  [19200/74682]\n",
      "loss: 0.759133  [25600/74682]\n",
      "loss: 0.539279  [32000/74682]\n",
      "loss: 0.450879  [38400/74682]\n",
      "loss: 0.413745  [44800/74682]\n",
      "loss: 0.604896  [51200/74682]\n",
      "loss: 0.340691  [57600/74682]\n",
      "loss: 0.357960  [64000/74682]\n",
      "loss: 0.671946  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.254403 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.462978  [    0/74682]\n",
      "loss: 0.363560  [ 6400/74682]\n",
      "loss: 0.518997  [12800/74682]\n",
      "loss: 0.230862  [19200/74682]\n",
      "loss: 0.510892  [25600/74682]\n",
      "loss: 0.223855  [32000/74682]\n",
      "loss: 0.592615  [38400/74682]\n",
      "loss: 0.920492  [44800/74682]\n",
      "loss: 0.411854  [51200/74682]\n",
      "loss: 0.504059  [57600/74682]\n",
      "loss: 0.369594  [64000/74682]\n",
      "loss: 0.518459  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.235538 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.472099  [    0/74682]\n",
      "loss: 0.202449  [ 6400/74682]\n",
      "loss: 0.551317  [12800/74682]\n",
      "loss: 0.192701  [19200/74682]\n",
      "loss: 0.562576  [25600/74682]\n",
      "loss: 0.435383  [32000/74682]\n",
      "loss: 0.410991  [38400/74682]\n",
      "loss: 0.266351  [44800/74682]\n",
      "loss: 0.353773  [51200/74682]\n",
      "loss: 0.708472  [57600/74682]\n",
      "loss: 0.304265  [64000/74682]\n",
      "loss: 0.282347  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.214053 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.207825  [    0/74682]\n",
      "loss: 0.273021  [ 6400/74682]\n",
      "loss: 0.370842  [12800/74682]\n",
      "loss: 0.246341  [19200/74682]\n",
      "loss: 0.581922  [25600/74682]\n",
      "loss: 0.366122  [32000/74682]\n",
      "loss: 0.383549  [38400/74682]\n",
      "loss: 0.289794  [44800/74682]\n",
      "loss: 0.235614  [51200/74682]\n",
      "loss: 0.368277  [57600/74682]\n",
      "loss: 0.303523  [64000/74682]\n",
      "loss: 0.351646  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.209918 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.216723  [    0/74682]\n",
      "loss: 0.282816  [ 6400/74682]\n",
      "loss: 0.437886  [12800/74682]\n",
      "loss: 0.251986  [19200/74682]\n",
      "loss: 0.164432  [25600/74682]\n",
      "loss: 0.424817  [32000/74682]\n",
      "loss: 0.354367  [38400/74682]\n",
      "loss: 0.283032  [44800/74682]\n",
      "loss: 0.321778  [51200/74682]\n",
      "loss: 0.334034  [57600/74682]\n",
      "loss: 0.158279  [64000/74682]\n",
      "loss: 0.236604  [70400/74682]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.207187 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
