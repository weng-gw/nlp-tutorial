{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Embedding bag\n",
    "We will use MLP to perfomr sentiment analysis for Twitter data. Instead of using TF-IDF or other features as input, we will convert each word/token of the text into a word embedding and average all embeddings from the same tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"datasets/twitter_sentiment_analysis/twitter_training.csv\"\n",
    "train_data = pd.read_csv(train_data_path,header=None)\n",
    "train_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]\n",
    "\n",
    "test_data_path = \"datasets/twitter_sentiment_analysis/twitter_validation.csv\"\n",
    "test_data = pd.read_csv(test_data_path,header=None)\n",
    "test_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inlcude Only \"Positive\" and \"Negatvie\" twitts to form a binary classification problem\n",
    "## Label Positve as 1 and Negative as 0\n",
    "train_data = train_data[train_data.sentiment.isin([\"Positive\",\"Negative\"])]\n",
    "train_data[\"label\"] = train_data.sentiment.map({\"Positive\":1, \"Negative\":0})\n",
    "test_data = test_data[test_data.sentiment.isin([\"Positive\",\"Negative\"])]\n",
    "test_data[\"label\"] = test_data.sentiment.map({\"Positive\":1, \"Negative\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "##device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "device = torch.device(\"cpu\") ## currently mps does not support EmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data[\"Tweet_content\"].map(str)), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[716, 216]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"hello\", \"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset:\n",
    "    def __init__(self, texts, label):\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.label[idx]\n",
    "\n",
    "train_dataset = TwitterDataset(train_data['Tweet_content'].map(str).values,train_data[\"label\"].values)\n",
    "test_dataset = TwitterDataset(test_data['Tweet_content'].map(str).values,test_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list,dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list=  torch.cat(text_list)\n",
    "    return label_list, text_list, offsets\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "    def forward(self,x,offsets):\n",
    "        embeded = self.embedding(x, offsets)\n",
    "        return self.fc(embeded)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassification(len(vocab), embed_dim = 64, num_class = 2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, print_per_batches=50):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (y, X, o) in enumerate(dataloader):\n",
    "        X, y, o = X.to(device), y.to(device), o.to(device)\n",
    "\n",
    "        pred = model(X, o)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_per_batches ==0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (y, X, o) in enumerate(dataloader):\n",
    "            X, y, o = X.to(device), y.to(device), o.to(device)\n",
    "            pred = model(X, o)\n",
    "            loss = loss_fn(pred,y)\n",
    "            test_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.699312  [    0/43374]\n",
      "loss: 0.604927  [69850/43374]\n",
      "loss: 0.435660  [138400/43374]\n",
      "loss: 0.577339  [275850/43374]\n",
      "loss: 0.338536  [294400/43374]\n",
      "loss: 0.405945  [359250/43374]\n",
      "loss: 0.236121  [392100/43374]\n",
      "loss: 0.230910  [455000/43374]\n",
      "loss: 0.287862  [517600/43374]\n",
      "loss: 0.300403  [634500/43374]\n",
      "loss: 0.285999  [748000/43374]\n",
      "loss: 0.304240  [771650/43374]\n",
      "loss: 0.335285  [1026600/43374]\n",
      "loss: 0.367649  [1062750/43374]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.122118 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.204199  [    0/43374]\n",
      "loss: 0.167257  [84300/43374]\n",
      "loss: 0.155036  [139000/43374]\n",
      "loss: 0.162015  [187950/43374]\n",
      "loss: 0.178462  [309800/43374]\n",
      "loss: 0.119915  [322500/43374]\n",
      "loss: 0.184415  [434400/43374]\n",
      "loss: 0.137225  [536900/43374]\n",
      "loss: 0.322225  [609600/43374]\n",
      "loss: 0.153902  [644850/43374]\n",
      "loss: 0.126318  [805000/43374]\n",
      "loss: 0.219035  [750750/43374]\n",
      "loss: 0.267717  [939000/43374]\n",
      "loss: 0.335995  [937950/43374]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.094282 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.143997  [    0/43374]\n",
      "loss: 0.159767  [84250/43374]\n",
      "loss: 0.071854  [119800/43374]\n",
      "loss: 0.126835  [213600/43374]\n",
      "loss: 0.213824  [283200/43374]\n",
      "loss: 0.109155  [446250/43374]\n",
      "loss: 0.130042  [419100/43374]\n",
      "loss: 0.252952  [575750/43374]\n",
      "loss: 0.089578  [425600/43374]\n",
      "loss: 0.143315  [632250/43374]\n",
      "loss: 0.097102  [692500/43374]\n",
      "loss: 0.280413  [776600/43374]\n",
      "loss: 0.161440  [864000/43374]\n",
      "loss: 0.105009  [978900/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.093297 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.048013  [    0/43374]\n",
      "loss: 0.067928  [70450/43374]\n",
      "loss: 0.100709  [136000/43374]\n",
      "loss: 0.163047  [174750/43374]\n",
      "loss: 0.085856  [289400/43374]\n",
      "loss: 0.094379  [329500/43374]\n",
      "loss: 0.092573  [532500/43374]\n",
      "loss: 0.123685  [476350/43374]\n",
      "loss: 0.087088  [502400/43374]\n",
      "loss: 0.047434  [580950/43374]\n",
      "loss: 0.066918  [735500/43374]\n",
      "loss: 0.077947  [972400/43374]\n",
      "loss: 0.098424  [954000/43374]\n",
      "loss: 0.092441  [973700/43374]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.090863 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.165814  [    0/43374]\n",
      "loss: 0.071095  [69400/43374]\n",
      "loss: 0.047561  [144300/43374]\n",
      "loss: 0.202719  [175950/43374]\n",
      "loss: 0.038143  [233800/43374]\n",
      "loss: 0.045275  [393250/43374]\n",
      "loss: 0.310129  [407100/43374]\n",
      "loss: 0.100140  [584500/43374]\n",
      "loss: 0.289508  [694000/43374]\n",
      "loss: 0.185211  [626400/43374]\n",
      "loss: 0.074610  [729500/43374]\n",
      "loss: 0.145250  [726550/43374]\n",
      "loss: 0.201363  [847800/43374]\n",
      "loss: 0.140836  [881400/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.092286 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.054375  [    0/43374]\n",
      "loss: 0.184220  [65050/43374]\n",
      "loss: 0.112066  [148100/43374]\n",
      "loss: 0.193367  [204900/43374]\n",
      "loss: 0.048316  [268800/43374]\n",
      "loss: 0.068970  [290500/43374]\n",
      "loss: 0.033810  [443400/43374]\n",
      "loss: 0.099688  [487900/43374]\n",
      "loss: 0.104981  [623200/43374]\n",
      "loss: 0.080683  [685350/43374]\n",
      "loss: 0.187951  [588500/43374]\n",
      "loss: 0.132873  [821150/43374]\n",
      "loss: 0.043956  [966000/43374]\n",
      "loss: 0.207710  [833950/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.107043 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.052052  [    0/43374]\n",
      "loss: 0.144364  [72400/43374]\n",
      "loss: 0.102139  [144700/43374]\n",
      "loss: 0.091307  [231000/43374]\n",
      "loss: 0.039594  [319600/43374]\n",
      "loss: 0.178805  [392250/43374]\n",
      "loss: 0.109290  [483900/43374]\n",
      "loss: 0.087532  [498050/43374]\n",
      "loss: 0.107233  [594400/43374]\n",
      "loss: 0.117822  [590850/43374]\n",
      "loss: 0.118114  [642000/43374]\n",
      "loss: 0.060934  [860200/43374]\n",
      "loss: 0.130943  [885600/43374]\n",
      "loss: 0.085531  [963300/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.112050 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.079468  [    0/43374]\n",
      "loss: 0.018474  [77500/43374]\n",
      "loss: 0.180739  [134400/43374]\n",
      "loss: 0.054556  [232500/43374]\n",
      "loss: 0.061352  [289800/43374]\n",
      "loss: 0.120364  [361250/43374]\n",
      "loss: 0.054616  [498600/43374]\n",
      "loss: 0.139916  [564550/43374]\n",
      "loss: 0.114841  [613600/43374]\n",
      "loss: 0.056018  [654750/43374]\n",
      "loss: 0.081262  [740500/43374]\n",
      "loss: 0.027646  [765600/43374]\n",
      "loss: 0.061361  [871800/43374]\n",
      "loss: 0.097310  [921050/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.110871 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.020847  [    0/43374]\n",
      "loss: 0.077208  [70700/43374]\n",
      "loss: 0.094164  [146200/43374]\n",
      "loss: 0.068975  [241500/43374]\n",
      "loss: 0.211928  [224000/43374]\n",
      "loss: 0.194932  [341750/43374]\n",
      "loss: 0.046332  [414600/43374]\n",
      "loss: 0.061425  [513100/43374]\n",
      "loss: 0.057892  [500000/43374]\n",
      "loss: 0.189048  [725400/43374]\n",
      "loss: 0.112552  [591000/43374]\n",
      "loss: 0.092310  [742500/43374]\n",
      "loss: 0.253560  [802200/43374]\n",
      "loss: 0.132715  [949650/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.119004 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.154777  [    0/43374]\n",
      "loss: 0.098804  [65050/43374]\n",
      "loss: 0.009123  [140100/43374]\n",
      "loss: 0.016439  [208800/43374]\n",
      "loss: 0.042069  [267800/43374]\n",
      "loss: 0.022441  [339250/43374]\n",
      "loss: 0.038848  [360600/43374]\n",
      "loss: 0.077390  [447650/43374]\n",
      "loss: 0.206012  [613200/43374]\n",
      "loss: 0.074540  [589050/43374]\n",
      "loss: 0.056495  [692500/43374]\n",
      "loss: 0.084584  [709500/43374]\n",
      "loss: 0.193088  [846600/43374]\n",
      "loss: 0.037000  [913900/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.129249 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
