{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Embedding bag\n",
    "We will use MLP to perfomr sentiment analysis for Twitter data. Instead of using TF-IDF or other features as input, we will convert each word/token of the text into a word embedding and average all embeddings from the same tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"datasets/twitter_sentiment_analysis/twitter_training.csv\"\n",
    "train_data = pd.read_csv(train_data_path,header=None)\n",
    "train_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]\n",
    "\n",
    "test_data_path = \"datasets/twitter_sentiment_analysis/twitter_validation.csv\"\n",
    "test_data = pd.read_csv(test_data_path,header=None)\n",
    "test_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inlcude Only \"Positive\" and \"Negatvie\" twitts to form a binary classification problem\n",
    "## Label Positve as 1 and Negative as 0\n",
    "train_data = train_data[train_data.sentiment.isin([\"Positive\",\"Negative\"])]\n",
    "train_data[\"label\"] = train_data.sentiment.map({\"Positive\":1, \"Negative\":0})\n",
    "test_data = test_data[test_data.sentiment.isin([\"Positive\",\"Negative\"])]\n",
    "test_data[\"label\"] = test_data.sentiment.map({\"Positive\":1, \"Negative\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "##device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac\n",
    "device = torch.device(\"cpu\") ## currently mps does not support EmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data[\"Tweet_content\"].map(str)), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[716, 216]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"hello\", \"world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset:\n",
    "    def __init__(self, texts, label):\n",
    "        self.texts = texts\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.label[idx]\n",
    "\n",
    "train_dataset = TwitterDataset(train_data['Tweet_content'].map(str).values,train_data[\"label\"].values)\n",
    "test_dataset = TwitterDataset(test_data['Tweet_content'].map(str).values,test_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list,dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list=  torch.cat(text_list)\n",
    "    return label_list, text_list, offsets\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "    def forward(self,x,offsets):\n",
    "        embeded = self.embedding(x, offsets)\n",
    "        return self.fc(embeded)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassification(len(vocab), embed_dim = 64, num_class = 2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, print_per_batches=50):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (y, X, o) in enumerate(dataloader):\n",
    "        X, y, o = X.to(device), y.to(device), o.to(device)\n",
    "\n",
    "        pred = model(X, o)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_per_batches ==0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (y, X, o) in enumerate(dataloader):\n",
    "            X, y, o = X.to(device), y.to(device), o.to(device)\n",
    "            pred = model(X, o)\n",
    "            loss = loss_fn(pred,y)\n",
    "            test_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.699312  [    0/43374]\n",
      "loss: 0.604927  [69850/43374]\n",
      "loss: 0.435660  [138400/43374]\n",
      "loss: 0.577339  [275850/43374]\n",
      "loss: 0.338536  [294400/43374]\n",
      "loss: 0.405945  [359250/43374]\n",
      "loss: 0.236121  [392100/43374]\n",
      "loss: 0.230910  [455000/43374]\n",
      "loss: 0.287862  [517600/43374]\n",
      "loss: 0.300403  [634500/43374]\n",
      "loss: 0.285999  [748000/43374]\n",
      "loss: 0.304240  [771650/43374]\n",
      "loss: 0.335285  [1026600/43374]\n",
      "loss: 0.367649  [1062750/43374]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.122118 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.204199  [    0/43374]\n",
      "loss: 0.167257  [84300/43374]\n",
      "loss: 0.155036  [139000/43374]\n",
      "loss: 0.162015  [187950/43374]\n",
      "loss: 0.178462  [309800/43374]\n",
      "loss: 0.119915  [322500/43374]\n",
      "loss: 0.184415  [434400/43374]\n",
      "loss: 0.137225  [536900/43374]\n",
      "loss: 0.322225  [609600/43374]\n",
      "loss: 0.153902  [644850/43374]\n",
      "loss: 0.126318  [805000/43374]\n",
      "loss: 0.219035  [750750/43374]\n",
      "loss: 0.267717  [939000/43374]\n",
      "loss: 0.335995  [937950/43374]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.094282 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.143997  [    0/43374]\n",
      "loss: 0.159767  [84250/43374]\n",
      "loss: 0.071854  [119800/43374]\n",
      "loss: 0.126835  [213600/43374]\n",
      "loss: 0.213824  [283200/43374]\n",
      "loss: 0.109155  [446250/43374]\n",
      "loss: 0.130042  [419100/43374]\n",
      "loss: 0.252952  [575750/43374]\n",
      "loss: 0.089578  [425600/43374]\n",
      "loss: 0.143315  [632250/43374]\n",
      "loss: 0.097102  [692500/43374]\n",
      "loss: 0.280413  [776600/43374]\n",
      "loss: 0.161440  [864000/43374]\n",
      "loss: 0.105009  [978900/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.093297 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.048013  [    0/43374]\n",
      "loss: 0.067928  [70450/43374]\n",
      "loss: 0.100709  [136000/43374]\n",
      "loss: 0.163047  [174750/43374]\n",
      "loss: 0.085856  [289400/43374]\n",
      "loss: 0.094379  [329500/43374]\n",
      "loss: 0.092573  [532500/43374]\n",
      "loss: 0.123685  [476350/43374]\n",
      "loss: 0.087088  [502400/43374]\n",
      "loss: 0.047434  [580950/43374]\n",
      "loss: 0.066918  [735500/43374]\n",
      "loss: 0.077947  [972400/43374]\n",
      "loss: 0.098424  [954000/43374]\n",
      "loss: 0.092441  [973700/43374]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.090863 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.165814  [    0/43374]\n",
      "loss: 0.071095  [69400/43374]\n",
      "loss: 0.047561  [144300/43374]\n",
      "loss: 0.202719  [175950/43374]\n",
      "loss: 0.038143  [233800/43374]\n",
      "loss: 0.045275  [393250/43374]\n",
      "loss: 0.310129  [407100/43374]\n",
      "loss: 0.100140  [584500/43374]\n",
      "loss: 0.289508  [694000/43374]\n",
      "loss: 0.185211  [626400/43374]\n",
      "loss: 0.074610  [729500/43374]\n",
      "loss: 0.145250  [726550/43374]\n",
      "loss: 0.201363  [847800/43374]\n",
      "loss: 0.140836  [881400/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.092286 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.054375  [    0/43374]\n",
      "loss: 0.184220  [65050/43374]\n",
      "loss: 0.112066  [148100/43374]\n",
      "loss: 0.193367  [204900/43374]\n",
      "loss: 0.048316  [268800/43374]\n",
      "loss: 0.068970  [290500/43374]\n",
      "loss: 0.033810  [443400/43374]\n",
      "loss: 0.099688  [487900/43374]\n",
      "loss: 0.104981  [623200/43374]\n",
      "loss: 0.080683  [685350/43374]\n",
      "loss: 0.187951  [588500/43374]\n",
      "loss: 0.132873  [821150/43374]\n",
      "loss: 0.043956  [966000/43374]\n",
      "loss: 0.207710  [833950/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.107043 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.052052  [    0/43374]\n",
      "loss: 0.144364  [72400/43374]\n",
      "loss: 0.102139  [144700/43374]\n",
      "loss: 0.091307  [231000/43374]\n",
      "loss: 0.039594  [319600/43374]\n",
      "loss: 0.178805  [392250/43374]\n",
      "loss: 0.109290  [483900/43374]\n",
      "loss: 0.087532  [498050/43374]\n",
      "loss: 0.107233  [594400/43374]\n",
      "loss: 0.117822  [590850/43374]\n",
      "loss: 0.118114  [642000/43374]\n",
      "loss: 0.060934  [860200/43374]\n",
      "loss: 0.130943  [885600/43374]\n",
      "loss: 0.085531  [963300/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.112050 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.079468  [    0/43374]\n",
      "loss: 0.018474  [77500/43374]\n",
      "loss: 0.180739  [134400/43374]\n",
      "loss: 0.054556  [232500/43374]\n",
      "loss: 0.061352  [289800/43374]\n",
      "loss: 0.120364  [361250/43374]\n",
      "loss: 0.054616  [498600/43374]\n",
      "loss: 0.139916  [564550/43374]\n",
      "loss: 0.114841  [613600/43374]\n",
      "loss: 0.056018  [654750/43374]\n",
      "loss: 0.081262  [740500/43374]\n",
      "loss: 0.027646  [765600/43374]\n",
      "loss: 0.061361  [871800/43374]\n",
      "loss: 0.097310  [921050/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.110871 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.020847  [    0/43374]\n",
      "loss: 0.077208  [70700/43374]\n",
      "loss: 0.094164  [146200/43374]\n",
      "loss: 0.068975  [241500/43374]\n",
      "loss: 0.211928  [224000/43374]\n",
      "loss: 0.194932  [341750/43374]\n",
      "loss: 0.046332  [414600/43374]\n",
      "loss: 0.061425  [513100/43374]\n",
      "loss: 0.057892  [500000/43374]\n",
      "loss: 0.189048  [725400/43374]\n",
      "loss: 0.112552  [591000/43374]\n",
      "loss: 0.092310  [742500/43374]\n",
      "loss: 0.253560  [802200/43374]\n",
      "loss: 0.132715  [949650/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.119004 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.154777  [    0/43374]\n",
      "loss: 0.098804  [65050/43374]\n",
      "loss: 0.009123  [140100/43374]\n",
      "loss: 0.016439  [208800/43374]\n",
      "loss: 0.042069  [267800/43374]\n",
      "loss: 0.022441  [339250/43374]\n",
      "loss: 0.038848  [360600/43374]\n",
      "loss: 0.077390  [447650/43374]\n",
      "loss: 0.206012  [613200/43374]\n",
      "loss: 0.074540  [589050/43374]\n",
      "loss: 0.056495  [692500/43374]\n",
      "loss: 0.084584  [709500/43374]\n",
      "loss: 0.193088  [846600/43374]\n",
      "loss: 0.037000  [913900/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.129249 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using nn.Embedding instead of nn.EmbeddingBag\n",
    "From previous implementation, we see a warning message that `nn.EmbeddingBad` currently does not support a mps backend. We will imlementation the model using `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.transforms import VocabTransform, ToTensor\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  ## Only works on M-Series Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data[\"Tweet_content\"].map(str)), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "totensor = ToTensor(padding_value=vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We redefine the collate function so that it will do dynamic padding\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for  _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = vocab(tokenizer(_text))\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list,dtype=torch.int64)\n",
    "    text_list=  totensor(text_list)\n",
    "    return  text_list, label_list\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model with `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class TextClassification(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, padding_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "    def forward(self,x):\n",
    "        embeded = self.embedding(x)\n",
    "        embeded_mean = embeded.mean(dim=1)\n",
    "        return self.fc(embeded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassification(len(vocab), embed_dim = 64, num_class = 2, padding_idx=vocab[\"<pad>\"]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, print_per_batches=50):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y  = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_per_batches ==0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred,y)\n",
    "            test_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.050813  [    0/43374]\n",
      "loss: 0.076309  [ 3200/43374]\n",
      "loss: 0.148553  [ 6400/43374]\n",
      "loss: 0.087332  [ 9600/43374]\n",
      "loss: 0.019943  [12800/43374]\n",
      "loss: 0.018043  [16000/43374]\n",
      "loss: 0.058363  [19200/43374]\n",
      "loss: 0.031814  [22400/43374]\n",
      "loss: 0.120483  [25600/43374]\n",
      "loss: 0.080587  [28800/43374]\n",
      "loss: 0.212798  [32000/43374]\n",
      "loss: 0.026256  [35200/43374]\n",
      "loss: 0.050686  [38400/43374]\n",
      "loss: 0.127219  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.098561 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.164729  [    0/43374]\n",
      "loss: 0.113378  [ 3200/43374]\n",
      "loss: 0.074571  [ 6400/43374]\n",
      "loss: 0.078975  [ 9600/43374]\n",
      "loss: 0.063279  [12800/43374]\n",
      "loss: 0.034681  [16000/43374]\n",
      "loss: 0.040806  [19200/43374]\n",
      "loss: 0.057081  [22400/43374]\n",
      "loss: 0.082453  [25600/43374]\n",
      "loss: 0.088137  [28800/43374]\n",
      "loss: 0.113274  [32000/43374]\n",
      "loss: 0.061517  [35200/43374]\n",
      "loss: 0.102048  [38400/43374]\n",
      "loss: 0.081739  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.099651 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.074787  [    0/43374]\n",
      "loss: 0.107183  [ 3200/43374]\n",
      "loss: 0.044370  [ 6400/43374]\n",
      "loss: 0.036080  [ 9600/43374]\n",
      "loss: 0.064433  [12800/43374]\n",
      "loss: 0.058928  [16000/43374]\n",
      "loss: 0.034866  [19200/43374]\n",
      "loss: 0.088842  [22400/43374]\n",
      "loss: 0.019297  [25600/43374]\n",
      "loss: 0.014771  [28800/43374]\n",
      "loss: 0.028448  [32000/43374]\n",
      "loss: 0.085893  [35200/43374]\n",
      "loss: 0.061060  [38400/43374]\n",
      "loss: 0.252869  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.097876 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.022500  [    0/43374]\n",
      "loss: 0.041447  [ 3200/43374]\n",
      "loss: 0.093462  [ 6400/43374]\n",
      "loss: 0.051830  [ 9600/43374]\n",
      "loss: 0.059815  [12800/43374]\n",
      "loss: 0.047781  [16000/43374]\n",
      "loss: 0.052031  [19200/43374]\n",
      "loss: 0.032158  [22400/43374]\n",
      "loss: 0.026482  [25600/43374]\n",
      "loss: 0.034610  [28800/43374]\n",
      "loss: 0.046163  [32000/43374]\n",
      "loss: 0.040393  [35200/43374]\n",
      "loss: 0.115273  [38400/43374]\n",
      "loss: 0.046529  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.099197 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.049187  [    0/43374]\n",
      "loss: 0.041393  [ 3200/43374]\n",
      "loss: 0.082876  [ 6400/43374]\n",
      "loss: 0.024694  [ 9600/43374]\n",
      "loss: 0.106051  [12800/43374]\n",
      "loss: 0.083441  [16000/43374]\n",
      "loss: 0.035810  [19200/43374]\n",
      "loss: 0.066481  [22400/43374]\n",
      "loss: 0.055246  [25600/43374]\n",
      "loss: 0.084907  [28800/43374]\n",
      "loss: 0.074394  [32000/43374]\n",
      "loss: 0.078898  [35200/43374]\n",
      "loss: 0.067145  [38400/43374]\n",
      "loss: 0.070880  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.101343 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.093445  [    0/43374]\n",
      "loss: 0.088514  [ 3200/43374]\n",
      "loss: 0.048351  [ 6400/43374]\n",
      "loss: 0.060824  [ 9600/43374]\n",
      "loss: 0.046911  [12800/43374]\n",
      "loss: 0.115602  [16000/43374]\n",
      "loss: 0.069143  [19200/43374]\n",
      "loss: 0.044098  [22400/43374]\n",
      "loss: 0.039388  [25600/43374]\n",
      "loss: 0.074815  [28800/43374]\n",
      "loss: 0.018940  [32000/43374]\n",
      "loss: 0.096393  [35200/43374]\n",
      "loss: 0.035934  [38400/43374]\n",
      "loss: 0.109997  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.103147 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.106990  [    0/43374]\n",
      "loss: 0.048430  [ 3200/43374]\n",
      "loss: 0.080205  [ 6400/43374]\n",
      "loss: 0.039398  [ 9600/43374]\n",
      "loss: 0.169545  [12800/43374]\n",
      "loss: 0.039618  [16000/43374]\n",
      "loss: 0.086230  [19200/43374]\n",
      "loss: 0.047200  [22400/43374]\n",
      "loss: 0.037105  [25600/43374]\n",
      "loss: 0.100307  [28800/43374]\n",
      "loss: 0.034416  [32000/43374]\n",
      "loss: 0.059503  [35200/43374]\n",
      "loss: 0.040115  [38400/43374]\n",
      "loss: 0.028154  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.107612 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.028570  [    0/43374]\n",
      "loss: 0.114973  [ 3200/43374]\n",
      "loss: 0.035206  [ 6400/43374]\n",
      "loss: 0.048227  [ 9600/43374]\n",
      "loss: 0.031588  [12800/43374]\n",
      "loss: 0.131468  [16000/43374]\n",
      "loss: 0.056536  [19200/43374]\n",
      "loss: 0.016297  [22400/43374]\n",
      "loss: 0.006067  [25600/43374]\n",
      "loss: 0.054107  [28800/43374]\n",
      "loss: 0.014647  [32000/43374]\n",
      "loss: 0.020046  [35200/43374]\n",
      "loss: 0.055300  [38400/43374]\n",
      "loss: 0.035695  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.110281 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.043868  [    0/43374]\n",
      "loss: 0.029414  [ 3200/43374]\n",
      "loss: 0.054379  [ 6400/43374]\n",
      "loss: 0.136061  [ 9600/43374]\n",
      "loss: 0.024541  [12800/43374]\n",
      "loss: 0.012074  [16000/43374]\n",
      "loss: 0.028159  [19200/43374]\n",
      "loss: 0.021897  [22400/43374]\n",
      "loss: 0.030000  [25600/43374]\n",
      "loss: 0.047454  [28800/43374]\n",
      "loss: 0.071706  [32000/43374]\n",
      "loss: 0.037848  [35200/43374]\n",
      "loss: 0.138895  [38400/43374]\n",
      "loss: 0.023812  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.107366 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.052611  [    0/43374]\n",
      "loss: 0.050672  [ 3200/43374]\n",
      "loss: 0.017733  [ 6400/43374]\n",
      "loss: 0.055822  [ 9600/43374]\n",
      "loss: 0.089662  [12800/43374]\n",
      "loss: 0.009625  [16000/43374]\n",
      "loss: 0.033145  [19200/43374]\n",
      "loss: 0.087674  [22400/43374]\n",
      "loss: 0.019208  [25600/43374]\n",
      "loss: 0.007369  [28800/43374]\n",
      "loss: 0.127183  [32000/43374]\n",
      "loss: 0.039385  [35200/43374]\n",
      "loss: 0.095982  [38400/43374]\n",
      "loss: 0.049692  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.113671 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.066174  [    0/43374]\n",
      "loss: 0.137081  [ 3200/43374]\n",
      "loss: 0.011255  [ 6400/43374]\n",
      "loss: 0.061364  [ 9600/43374]\n",
      "loss: 0.067112  [12800/43374]\n",
      "loss: 0.071777  [16000/43374]\n",
      "loss: 0.047492  [19200/43374]\n",
      "loss: 0.003691  [22400/43374]\n",
      "loss: 0.018071  [25600/43374]\n",
      "loss: 0.116626  [28800/43374]\n",
      "loss: 0.047670  [32000/43374]\n",
      "loss: 0.059146  [35200/43374]\n",
      "loss: 0.034622  [38400/43374]\n",
      "loss: 0.034452  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.112981 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.140162  [    0/43374]\n",
      "loss: 0.034384  [ 3200/43374]\n",
      "loss: 0.103754  [ 6400/43374]\n",
      "loss: 0.043212  [ 9600/43374]\n",
      "loss: 0.060548  [12800/43374]\n",
      "loss: 0.075804  [16000/43374]\n",
      "loss: 0.083145  [19200/43374]\n",
      "loss: 0.045521  [22400/43374]\n",
      "loss: 0.048096  [25600/43374]\n",
      "loss: 0.054143  [28800/43374]\n",
      "loss: 0.054755  [32000/43374]\n",
      "loss: 0.101993  [35200/43374]\n",
      "loss: 0.019950  [38400/43374]\n",
      "loss: 0.023625  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.117479 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.103322  [    0/43374]\n",
      "loss: 0.050410  [ 3200/43374]\n",
      "loss: 0.043783  [ 6400/43374]\n",
      "loss: 0.027139  [ 9600/43374]\n",
      "loss: 0.028752  [12800/43374]\n",
      "loss: 0.055313  [16000/43374]\n",
      "loss: 0.047877  [19200/43374]\n",
      "loss: 0.055706  [22400/43374]\n",
      "loss: 0.069222  [25600/43374]\n",
      "loss: 0.066647  [28800/43374]\n",
      "loss: 0.054686  [32000/43374]\n",
      "loss: 0.082721  [35200/43374]\n",
      "loss: 0.066282  [38400/43374]\n",
      "loss: 0.050232  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.118072 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.047006  [    0/43374]\n",
      "loss: 0.048137  [ 3200/43374]\n",
      "loss: 0.115397  [ 6400/43374]\n",
      "loss: 0.149845  [ 9600/43374]\n",
      "loss: 0.096441  [12800/43374]\n",
      "loss: 0.192107  [16000/43374]\n",
      "loss: 0.078226  [19200/43374]\n",
      "loss: 0.168020  [22400/43374]\n",
      "loss: 0.060617  [25600/43374]\n",
      "loss: 0.011815  [28800/43374]\n",
      "loss: 0.020955  [32000/43374]\n",
      "loss: 0.037163  [35200/43374]\n",
      "loss: 0.119254  [38400/43374]\n",
      "loss: 0.068590  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.123981 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.022018  [    0/43374]\n",
      "loss: 0.081735  [ 3200/43374]\n",
      "loss: 0.078609  [ 6400/43374]\n",
      "loss: 0.030938  [ 9600/43374]\n",
      "loss: 0.061030  [12800/43374]\n",
      "loss: 0.033767  [16000/43374]\n",
      "loss: 0.066111  [19200/43374]\n",
      "loss: 0.092294  [22400/43374]\n",
      "loss: 0.036142  [25600/43374]\n",
      "loss: 0.076941  [28800/43374]\n",
      "loss: 0.064099  [32000/43374]\n",
      "loss: 0.053289  [35200/43374]\n",
      "loss: 0.078171  [38400/43374]\n",
      "loss: 0.123037  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.128111 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.061833  [    0/43374]\n",
      "loss: 0.069083  [ 3200/43374]\n",
      "loss: 0.034739  [ 6400/43374]\n",
      "loss: 0.023607  [ 9600/43374]\n",
      "loss: 0.037908  [12800/43374]\n",
      "loss: 0.034352  [16000/43374]\n",
      "loss: 0.046022  [19200/43374]\n",
      "loss: 0.119073  [22400/43374]\n",
      "loss: 0.020069  [25600/43374]\n",
      "loss: 0.038322  [28800/43374]\n",
      "loss: 0.065176  [32000/43374]\n",
      "loss: 0.117956  [35200/43374]\n",
      "loss: 0.052064  [38400/43374]\n",
      "loss: 0.088316  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.129112 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.057170  [    0/43374]\n",
      "loss: 0.036212  [ 3200/43374]\n",
      "loss: 0.044121  [ 6400/43374]\n",
      "loss: 0.037948  [ 9600/43374]\n",
      "loss: 0.038150  [12800/43374]\n",
      "loss: 0.043855  [16000/43374]\n",
      "loss: 0.055547  [19200/43374]\n",
      "loss: 0.059110  [22400/43374]\n",
      "loss: 0.064932  [25600/43374]\n",
      "loss: 0.039756  [28800/43374]\n",
      "loss: 0.033521  [32000/43374]\n",
      "loss: 0.040828  [35200/43374]\n",
      "loss: 0.119284  [38400/43374]\n",
      "loss: 0.089669  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.129834 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.037987  [    0/43374]\n",
      "loss: 0.035595  [ 3200/43374]\n",
      "loss: 0.073474  [ 6400/43374]\n",
      "loss: 0.012666  [ 9600/43374]\n",
      "loss: 0.068515  [12800/43374]\n",
      "loss: 0.048360  [16000/43374]\n",
      "loss: 0.018797  [19200/43374]\n",
      "loss: 0.072928  [22400/43374]\n",
      "loss: 0.104240  [25600/43374]\n",
      "loss: 0.095397  [28800/43374]\n",
      "loss: 0.053839  [32000/43374]\n",
      "loss: 0.059239  [35200/43374]\n",
      "loss: 0.071075  [38400/43374]\n",
      "loss: 0.084717  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.131886 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.006942  [    0/43374]\n",
      "loss: 0.117631  [ 3200/43374]\n",
      "loss: 0.038518  [ 6400/43374]\n",
      "loss: 0.014338  [ 9600/43374]\n",
      "loss: 0.078615  [12800/43374]\n",
      "loss: 0.046757  [16000/43374]\n",
      "loss: 0.046393  [19200/43374]\n",
      "loss: 0.063412  [22400/43374]\n",
      "loss: 0.021761  [25600/43374]\n",
      "loss: 0.137102  [28800/43374]\n",
      "loss: 0.064215  [32000/43374]\n",
      "loss: 0.069133  [35200/43374]\n",
      "loss: 0.065440  [38400/43374]\n",
      "loss: 0.025961  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.132787 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.009624  [    0/43374]\n",
      "loss: 0.032780  [ 3200/43374]\n",
      "loss: 0.070225  [ 6400/43374]\n",
      "loss: 0.056542  [ 9600/43374]\n",
      "loss: 0.116499  [12800/43374]\n",
      "loss: 0.036042  [16000/43374]\n",
      "loss: 0.079467  [19200/43374]\n",
      "loss: 0.103813  [22400/43374]\n",
      "loss: 0.071065  [25600/43374]\n",
      "loss: 0.030826  [28800/43374]\n",
      "loss: 0.046386  [32000/43374]\n",
      "loss: 0.048269  [35200/43374]\n",
      "loss: 0.036222  [38400/43374]\n",
      "loss: 0.042161  [41600/43374]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.138619 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
