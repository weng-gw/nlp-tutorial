{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data preprocessing\n",
    "We know text preprocessing is the first step and an crucial part of NLP. Although nowdays many deep learning based NLP models have packages coming with built-in text preprocessing pipelines, it's still meaningful for us to understand the process of text data preprocessing which often times we will have to do by ourselves in some classical NLP tasks, e.g., sentiment analysis, topic modeling.\n",
    "\n",
    "In this tutorial, we use the [Twitter Sentiment Anlysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis) data as an exmaple to illustrate the general process of text preprocessing using two NLP packages, NLTK and Spacy, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74682 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tweet_ID       entity sentiment  \\\n",
       "0          2401  Borderlands  Positive   \n",
       "1          2401  Borderlands  Positive   \n",
       "2          2401  Borderlands  Positive   \n",
       "3          2401  Borderlands  Positive   \n",
       "4          2401  Borderlands  Positive   \n",
       "...         ...          ...       ...   \n",
       "74677      9200       Nvidia  Positive   \n",
       "74678      9200       Nvidia  Positive   \n",
       "74679      9200       Nvidia  Positive   \n",
       "74680      9200       Nvidia  Positive   \n",
       "74681      9200       Nvidia  Positive   \n",
       "\n",
       "                                           Tweet_content  \n",
       "0      im getting on borderlands and i will murder yo...  \n",
       "1      I am coming to the borders and I will kill you...  \n",
       "2      im getting on borderlands and i will kill you ...  \n",
       "3      im coming on borderlands and i will murder you...  \n",
       "4      im getting on borderlands 2 and i will murder ...  \n",
       "...                                                  ...  \n",
       "74677  Just realized that the Windows partition of my...  \n",
       "74678  Just realized that my Mac window partition is ...  \n",
       "74679  Just realized the windows partition of my Mac ...  \n",
       "74680  Just realized between the windows partition of...  \n",
       "74681  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[74682 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"datasets/twitter_sentiment_analysis/twitter_training.csv\"\n",
    "train_data = pd.read_csv(data_path,header=None)\n",
    "train_data.columns = [\"Tweet_ID\",\"entity\",\"sentiment\",\"Tweet_content\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      22542\n",
       "Positive      20832\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is not the first time that the EU Commission has taken such a step.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_id = np.random.randint(0, train_data.shape[0]-1)\n",
    "train_data.iloc[rand_id,][\"Tweet_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General steps of preprocessing\n",
    "\n",
    "Generally, we follow the below steps to perform preprocessing for text data\n",
    "\n",
    "1. Remove special strings or characters, e.g., Url, emoji, Twitter marks, styles; Note what to remove is a case by case question. For example, emoji can reveal emotions and actually could be imporant in sentiment analysis.\n",
    "2. Tokenizing the string. This is the process of splitting strings into chunks (words, punctuations)\n",
    "3. Lowercasing\n",
    "4. Remoing stop words and punctuations. Stop words are words that don't add significant meaning to the text. Punctuations are special characters that help to organize the structure of sentences, i.e., \",\", \".\", \"?\". But sometimes, there are strings of punctuations that contain meanings (serve like emojis) and should be retained, i.e., \":)\". \n",
    "5. Stemming/Lemmetization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming VS Lemmetization\n",
    "\n",
    "**Stemming** is  the process of reducing infected words to their stem. It is the process of removing the last few characters of a given word, to obtain a shorter form, even if that form doesn’t have any meaning.\n",
    "\n",
    "For example, after stemming, both \"History\" and \"Historical\" become \"Histori\" which has no meaning, or does not exist in English. \"Finally\" and \"Final\" become \"Fina\". \n",
    "\n",
    "The goal is to get the base form of similar words.\n",
    "\n",
    "**Lemmetization** has the same goal as stemming but overcomes the drawbacks of stemming. In stemming, for some words, it may not give may not give meaningful representation such as “Histori”. Here, lemmatization comes into picture as it gives meaningful word.\n",
    "\n",
    "Lemmatization takes more time as compared to stemming because it finds meaningful word/ representation. Stemming just needs to get a base word and therefore takes less time.\n",
    "\n",
    "Stemming has its application in Sentiment Analysis while Lemmatization has its application in Chatbots, human-answering.\n",
    "\n",
    "### Summary\n",
    "\t\n",
    "**Stemming** is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\t\n",
    "\n",
    "For instance, stemming the word ‘Caring‘ would return ‘Car‘.\n",
    "\n",
    "Stemming is used in case of large dataset where performance is an issue.\n",
    "\n",
    "**Lemmatization** considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
    "\n",
    "For instance, lemmatizing the word ‘Caring‘ would return ‘Care‘.\n",
    "\n",
    "Lemmatization is computationally expensive since it involves look-up tables and what not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/wgw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/wgw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/wgw/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is not the first time that the EU Commission has taken such a step.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = train_data.iloc[rand_id,][\"Tweet_content\"]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hyperlinks, Twitter marks and styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not the first time that the EU Commission has taken such a step.\n"
     ]
    }
   ],
   "source": [
    "# remove old sytle retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+','', tweet)\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the string\n",
    "\n",
    "We do tokenizing and lowercasing in the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'not', 'the', 'first', 'time', 'that', 'the', 'eu', 'commission', 'has', 'taken', 'such', 'a', 'step', '.']\n"
     ]
    }
   ],
   "source": [
    "# instantiate tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop workds and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'eu', 'commission', 'taken', 'step']\n"
     ]
    }
   ],
   "source": [
    "tweets_clean=[]\n",
    "for word in tweet_tokens:\n",
    "    if (word not in stopwords_english) and (word not in string.punctuation):\n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'eu', 'commiss', 'taken', 'step']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "tweets_stem = [stemmer.stem(w) for w in tweets_clean]\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'eu', 'commission', 'taken', 'step']\n"
     ]
    }
   ],
   "source": [
    "lemmertizer = WordNetLemmatizer()\n",
    "\n",
    "tweets_lem = [lemmertizer.lemmatize(w) for w in tweets_clean]\n",
    "print(tweets_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, tokenizer=nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True), stopwords = stopwords.words('english'), punctuation = string.punctuation, stemmer = nltk.stem.PorterStemmer(), lemmertizer = None):\n",
    "    tweet = str(tweet)\n",
    "    # remove old sytle retweet text \"RT\"\n",
    "    tweet2 = re.sub(r'^RT[\\s]+','', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "    # Tokenize\n",
    "    tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "    # remove stopworks and punctuation\n",
    "    tweets_clean=[]\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords) and (word not in punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    \n",
    "    # stemming or lemmetization\n",
    "    if stemmer:\n",
    "        tweets_clean = [stemmer.stem(w) for w in tweets_clean]\n",
    "    else:\n",
    "        tweets_clean = [lemmertizer.lemmatize(w) for w in tweets_clean]\n",
    "    \n",
    "    return tweets_clean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [im, get, borderland, murder]\n",
       "1                                     [come, border, kill]\n",
       "2                              [im, get, borderland, kill]\n",
       "3                           [im, come, borderland, murder]\n",
       "4                         [im, get, borderland, 2, murder]\n",
       "                               ...                        \n",
       "74677    [realiz, window, partit, mac, like, 6, year, b...\n",
       "74678    [realiz, mac, window, partit, 6, year, behind,...\n",
       "74679    [realiz, window, partit, mac, 6, year, behind,...\n",
       "74680    [realiz, window, partit, mac, like, 6, year, b...\n",
       "74681    [like, window, partit, mac, like, 6, year, beh...\n",
       "Name: Tweet_content, Length: 74682, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Tweet_content.apply(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Preprocessing Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not the first time that the EU Commission has taken such a step.\n"
     ]
    }
   ],
   "source": [
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is not the first time that the EU Commission has taken such a step.\n"
     ]
    }
   ],
   "source": [
    "# remove old sytle retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+','', tweet)\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'eu', 'commission', 'taken', 'step']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(tweet2)\n",
    "## tokens\n",
    "print([token.text.lower() for token in doc if (not token.is_stop) and (not token.is_punct) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'eu', 'commission', 'take', 'step']\n"
     ]
    }
   ],
   "source": [
    "## lemmatization\n",
    "print([token.lemma_.lower() for token in doc if (not token.is_stop) and (not token.is_punct) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet_spacy(tweet, lemmetizer=False):\n",
    "    # remove old sytle retweet text \"RT\"\n",
    "    tweet2 = re.sub(r'^RT[\\s]+','', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "    doc = nlp(tweet2)\n",
    "    # remove stopworks and punctuation\n",
    "    if lemmetizer:\n",
    "        return [token.lemma_.lower() for token in doc if (not token.is_stop) and (not token.is_punct) ]\n",
    "    else:\n",
    "        return [token.text.lower() for token in doc if (not token.is_stop) and (not token.is_punct) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [im, get, borderland, murder]\n",
       "1                [come, border, kill]\n",
       "2         [im, get, borderland, kill]\n",
       "3      [im, come, borderland, murder]\n",
       "4    [im, get, borderland, 2, murder]\n",
       "Name: Tweet_content, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[:5].Tweet_content.apply(process_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
